{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Coding AI Agents in Python\n",
    "\n",
    "#### By Pedro Izquierdo Lehmann\n",
    "\n",
    "Welcome to this hands-on introduction to **LangChain**! This notebook will guide you through building intelligent AI agents that can use tools, remember conversations, and make decisions autonomously.\n",
    "\n",
    "**What is LangChain?**\n",
    "LangChain is a framework for developing applications powered by language models. With it you can build explicit **chains**, which is an abstraction of an algorithm involving LLMs calls. Also, LangChain promotes implicit chains: instead of just asking an LLM questions, you can give it **tools** to use, and it will intelligently decide when and how to use them to answer your questions, instead of writing complex routing logic. \n",
    "\n",
    "LangChain works with the abstraction of the objects involved in the agentic system, such as\n",
    "\n",
    "- **Chains**: Abstraction of an algorithm involving multiple steps; a reusable workflow.\n",
    "- **Agents**: Abstraction of an LLM model equipped with tools, which can decide which tools/steps to run (an implicit chain).\n",
    "- **Tools**: Wrapped Python functions so the agent can call them.\n",
    "- **Memory/State**: Abstraction of context across conversation.\n",
    "\n",
    "LangChain orders these in **layers** of abstraction, so you can start simple and add power only when you need it. Each layer builds on the previous one. This notebook follows that same progression: we start with tools, then add memory, context, and structured outputs.\n",
    "\n",
    "**Content:**\n",
    "- Creating your first AI agent\n",
    "- Building custom tools for agents to use\n",
    "- Adding memory so agents remember past conversations\n",
    "- Using structured output for consistent responses\n",
    "- Context-aware tools that access user information\n",
    "- Best practices for production-ready agents\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (0. Environment Setup)\n",
    "\n",
    "Before starting, you need to set up a Python virtual environment and install all required dependencies. Follow these steps:\n",
    "\n",
    "#### 1. Create a Virtual Environment\n",
    "\n",
    "Open your terminal and navigate to the **directory containing this notebook**, then run:\n",
    "\n",
    "```bash\n",
    "python3 -m venv lang-chain\n",
    "```\n",
    "\n",
    "This creates a virtual environment in a folder called `lang-chain`.\n",
    "\n",
    "#### 2. Activate the Virtual Environment\n",
    "\n",
    "**On macOS/Linux:**\n",
    "```bash\n",
    "source lang-chain/bin/activate\n",
    "```\n",
    "\n",
    "**On Windows:**\n",
    "```bash\n",
    "lang-chain\\Scripts\\activate\n",
    "```\n",
    "\n",
    "You should see `(lang-chain)` at the beginning of your terminal prompt, indicating the virtual environment is active.\n",
    "\n",
    "#### 3. Install Required Dependencies\n",
    "\n",
    "With the virtual environment activated, install all necessary packages:\n",
    "\n",
    "```bash\n",
    "pip install langchain langgraph langchain-anthropic langchain-openai jupyter ipykernel\n",
    "```\n",
    "\n",
    "This will install:\n",
    "- `langchain` - The core LangChain framework\n",
    "- `langgraph` - For building stateful agent workflows and checkpointers\n",
    "- `langchain-anthropic` - Anthropic (Claude) model provider\n",
    "- `langchain-openai` - OpenAI model provider\n",
    "- `jupyter` - Jupyter notebook environment\n",
    "- `ipykernel` - Jupyter kernel for the virtual environment\n",
    "\n",
    "Register the virtual environment as a Jupyter kernel:\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name=lang-chain --display-name \"Python (lang-chain)\"\n",
    "```\n",
    "\n",
    "This ensures Jupyter can use your virtual environment's Python interpreter.\n",
    "\n",
    "#### 4. Start Jupyter Notebook\n",
    "\n",
    "We recommend two options to run the notebook:\n",
    "\n",
    "**Jupyter Notebook:**\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "This will open Jupyter in your web browser. Navigate to and open this notebook (`LangChain.ipynb`).\n",
    "\n",
    "**Code Editor like VS Code or Cursor:**\n",
    "\n",
    "1. Open the notebook file (`LangChain.ipynb`) in your code editor\n",
    "2. The editor should automatically detect it as a Jupyter notebook\n",
    "3. When prompted to select a kernel, choose **Python (lang-chain)** from the list\n",
    "4. If the kernel doesn't appear, you may need to refresh the kernel list or ensure the virtual environment is properly registered\n",
    "\n",
    "#### 5. Deactivate\n",
    "\n",
    "Don't forget to deactivate the virtual environment when you're done working with the following command:\n",
    "\n",
    "```bash\n",
    "deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m venv lang-chain\n",
    "# source lang-chain/bin/activate\n",
    "# lang-chain\\Scripts\\activate\n",
    "# pip install langchain langgraph langchain-anthropic langchain-openai jupyter ipykernel\n",
    "# python -m ipykernel install --user --name=lang-chain --display-name \"Python (lang-chain)\"\n",
    "# jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "# Set API key\n",
    "api_key = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chains\n",
    "\n",
    "A **chain** is a sequence of steps (prompts, tools, or other chains) connected into a **single reusable pipeline**.\n",
    "\n",
    "- Think of it as a recipe: each step transforms the input and passes it to the next.\n",
    "- Chains can be simple (prompt -> LLM) or complex (multi-step reasoning + tools).\n",
    "- Agents *use* chains internally, but chains are **deterministic**: the steps are predefined.\n",
    "\n",
    "LangChain lets you build **chains explicitly** (deterministic pipelines) or **implicitly** through agents (dynamic pipelines).\n",
    "\n",
    "- **Explicit chain:** You wire together steps (prompt → model → parsing). The flow is fixed and repeatable.\n",
    "- **Agentic (implicit) chain:** The model decides which steps/tools to run at runtime. The flow can vary across calls. \n",
    "\n",
    "> **Note**: The cool thing about agent chains is that instead of just asking an LLM a question, you can give it **tools** to use, and it will decide when and how to use them to answer your question. For example, you don't need to write code that says \"if the user asks about weather, call the weather tool.\" The agent figures this out on its own!\n",
    "\n",
    "Below is an example of an explicit chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: options pricing\n",
      "- Options pricing involves determining the fair value of a financial derivative based on the underlying asset's price, volatility, time to expiration, and other factors.  \n",
      "- The Black-Scholes model is one of the most widely used methods for calculating the theoretical price of European-style options.  \n",
      "- Market prices of options can deviate from their theoretical values due to supply and demand dynamics, liquidity, and market sentiment.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Explicit chain: fixed four-step pipeline\n",
    "# Step 1: Prompt template\n",
    "chain_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\", # sets the global rule for output format: title line + exactly three bullets. It’s treated as higher‑priority instructions.\n",
    "        \"Return output with first line 'Title: {topic}', followed by exactly three bullet points.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\", # supplies the task input (the specific topic) and adds a content constraint (bullets must be full sentences).\n",
    "        \"Topic: {topic}. Each bullet must be a complete sentence.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Step 2: Model call\n",
    "chain_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Step 3: Parse to string\n",
    "parser = StrOutputParser() # converts the raw LLM output into a string.\n",
    "\n",
    "# Step 4: Deterministic formatting\n",
    "format_output = RunnableLambda(\n",
    "    lambda s: \"\\n\".join(\n",
    "        [line for line in [\n",
    "            (\"Title: options pricing\" if not s.strip().split(\"\\n\")[0].startswith(\"Title:\") else s.strip().split(\"\\n\")[0]),\n",
    "            *[\n",
    "                (line if line.strip().startswith(\"-\") else f\"- {line.strip()}\")\n",
    "                for line in s.strip().split(\"\\n\")[1:]\n",
    "                if line.strip()\n",
    "            ][:3]\n",
    "        ] if line]\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = chain_prompt | chain_model | parser | format_output\n",
    "result = chain.invoke({\"topic\": \"options pricing\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build an Explicit Chain\n",
    "\n",
    "Create a chain that produces **three bullet points** about a finance topic. Use an explicit prompt + model pipeline, then invoke it.\n",
    "\n",
    "Hint: Use `ChatPromptTemplate`, compose with `|`, and access `response.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: risk-neutral pricing\n",
      "\n",
      "- Risk-neutral pricing is a method used in financial mathematics to determine the fair value of derivatives by assuming investors are indifferent to risk.  \n",
      "- This approach involves calculating the expected payoff of a financial instrument under a risk-neutral measure and discounting it at the risk-free rate.  \n",
      "- Risk-neutral pricing simplifies the valuation process by eliminating the need to consider individual risk preferences and market imperfections.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# EXERCISE: Build an explicit chain\n",
    "# 1. Create a ChatPromptTemplate with a {topic} variable\n",
    "# 2. Initialize a model with temperature=0\n",
    "# 3. Compose the chain with |\n",
    "# 4. Invoke it with topic=\"risk-neutral pricing\"\n",
    "# 5. Print response.content\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\", \n",
    "        \"Return output with first line 'Title: {topic}', followed by exactly three bullet points.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Topic: {topic}. Each bullet must be a complete sentence.\"\n",
    "    )\n",
    "])  # TODO: Fill this in\n",
    "model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature = 0, api_key = api_key)  # TODO: Fill this in\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser # TODO: Fill this in\n",
    "response = chain.invoke({\"topic\": \"risk-neutral pricing\"})  # TODO: Fill this in\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following will introduce chains implicitly as we build tool-driven workflows, then show how agents extend them with decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating an Agent\n",
    "\n",
    "Let's start by creating a simple agent. You can think of an agent as a **chain with decision-making**: it interprets the user input, decides which tools to call (if any), and produces a final response.\n",
    "\n",
    "An agent needs:\n",
    "1. A **model** (the LLM that does the thinking)\n",
    "2. **Tools** (functions the agent can call)\n",
    "3. A **system prompt** (instructions for the agent)\n",
    "\n",
    "Here's a concrete example of how to create and use an agent with a time tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: The current time in UTC is 16:18 on January 16, 2026.\n"
     ]
    }
   ],
   "source": [
    "# Example: Create an agent with a time tool\n",
    "def get_current_time(timezone: str = \"UTC\") -> str:\n",
    "    \"\"\"Get the current time in a specified timezone.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"Current time in {timezone}: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Initialize the model\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Create the agent\n",
    "example_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_current_time],\n",
    "    system_prompt=\"You are a helpful time assistant\"\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "example_response = example_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what time is it?\"}]}\n",
    ")\n",
    "\n",
    "# Access the response\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Basic Agent\n",
    "\n",
    "Now it's your turn! Create your first agent following the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in San Francisco is always sunny!\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a simple tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "# EXERCISE: Create a basic agent with a weather tool\n",
    "# 1. Initialize a chat model\n",
    "# Hint: Use init_chat_model() from langchain.chat_models with model name \"gpt-4.1-nano-2025-04-14\" or \"gpt-4\"\n",
    "model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature = 0, api_key = api_key)  # TODO: Fill this in\n",
    "\n",
    "# 2. Create an agent using create_agent\n",
    "# Hint: Use create_agent() from langchain.agents with model, tools=[get_weather], and system_prompt=\"You are a helpful assistant\"\n",
    "agent = create_agent(\n",
    "    model = model,\n",
    "    tools = [get_weather],\n",
    "    system_prompt = \"You are a helpful assistant\"\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 3. Run the agent with a message asking about the weather in San Francisco\n",
    "# Hint: Use agent.invoke() with {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in San Francisco\"}]}\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in San Francisco?\"}]}\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# Print the response\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Custom Tools\n",
    "\n",
    "Tools are functions that agents can call. LangChain makes it easy to convert Python functions into tools using the `@tool` decorator. The `@tool` decorator:\n",
    "- Automatically extracts function name, description, and parameters\n",
    "- Makes the function available to the agent\n",
    "- Handles type validation and conversion\n",
    "\n",
    "> **Important**: The function's docstring becomes part of the agent's prompt! Make it descriptive so the agent knows when to use the tool.\n",
    "\n",
    "Here's a working example using different tools (string manipulation) to demonstrate the @tool decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: The reversed string is \"dlroW olleH\" and it contains 2 words.\n"
     ]
    }
   ],
   "source": [
    "# Example: Create tools for string manipulation (different from the calculator exercise)\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def reverse_string(text: str) -> str:\n",
    "    \"\"\"Reverse a string.\"\"\"\n",
    "    return text[::-1]\n",
    "\n",
    "@tool\n",
    "def uppercase_string(text: str) -> str:\n",
    "    \"\"\"Convert a string to uppercase.\"\"\"\n",
    "    return text.upper()\n",
    "\n",
    "@tool\n",
    "def count_words(text: str) -> int:\n",
    "    \"\"\"Count the number of words in a string.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "# Create an agent with these string manipulation tools\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "string_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[reverse_string, uppercase_string, count_words],\n",
    "    system_prompt=\"You are a helpful text processing assistant\"\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "example_response = string_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Reverse the string 'Hello World' and count its words\"}]}\n",
    ")\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create a LangChain tools\n",
    "\n",
    "Now it's your turn! Create your first LangChain tools following the syntaxis of the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, 15 plus 27 equals 42. Then, multiplying that by 3 gives 126.\n"
     ]
    }
   ],
   "source": [
    "### Exercise 2: Create Multiple Tools\n",
    "\n",
    "# EXERCISE: Create a calculator agent with multiple tools\n",
    "# 1. Create a tool for addition\n",
    "# Hint: Use @tool decorator from langchain.tools, function should take (a: float, b: float) and return a + b\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b  # TODO: Fill this in (define the function with @tool decorator)\n",
    "\n",
    "# 2. Create a tool for multiplication\n",
    "# Hint: Use @tool decorator, function should take (a: float, b: float) and return a * b\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return a * b  # TODO: Fill this in (define the function with @tool decorator)\n",
    "\n",
    "# 3. Create a tool for getting the square root\n",
    "# Hint: Use @tool decorator and import math, function should take (x: float) and return math.sqrt(x)\n",
    "import math\n",
    "@tool\n",
    "def sqrt(x: float) -> float:\n",
    "    \"\"\"Calculates the square root of a number.\"\"\"\n",
    "    return math.sqrt(x)  # TODO: Fill this in (define the function with @tool decorator)\n",
    "\n",
    "# 4. Create an agent with all three tools\n",
    "# Hint: Use create_agent() from langchain.agents with model, tools=[add, multiply, sqrt], and system_prompt=\"You are a helpful calculator assistant\"\n",
    "calculator_agent = create_agent(\n",
    "    model = model,\n",
    "    tools = [add, multiply, sqrt],\n",
    "    system_prompt = \"You are a helpful calculator assistant\"\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 5. Test your agent\n",
    "# Hint: Use calculator_agent.invoke() with {\"messages\": [{\"role\": \"user\", \"content\": \"What is 15 plus 27, then multiply that by 3?\"}]}\n",
    "response = calculator_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is 15 plus 27, then multiply that by 3?\"}]}\n",
    ")  # TODO: Fill this in\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Behavior**: The agent should:\n",
    "1. First call `add(15, 27)` to get 42\n",
    "2. Then call `multiply(42, 3)` to get 126\n",
    "3. Return the final answer\n",
    "\n",
    "This demonstrates that agents can **chain multiple tool calls** to solve complex problems! The agent automatically figures out the sequence of operations needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tools with Runtime Context\n",
    "\n",
    "Sometimes tools need access to runtime information (like user IDs, session data, etc.). LangChain provides `ToolRuntime` for this. `ToolRuntime` allows tools to access:\n",
    "- **Context**: Custom data passed when invoking the agent\n",
    "- **Memory**: Conversation history and state\n",
    "- **Configuration**: Runtime settings\n",
    "\n",
    "> **Note**: The `ToolRuntime` parameter is automatically injected by LangChain. You don't pass it when calling the tool - LangChain handles that for you!\n",
    "\n",
    "Here is an example that uses `ToolRuntime` to build Context-Aware Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: Based on your preferences, I recommend exploring items in blue color. Would you like some specific suggestions or categories to consider?\n"
     ]
    }
   ],
   "source": [
    "# # Example: Context-aware tool for user preferences (different from greeting exercise)\n",
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Define a context schema with user preferences\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "    favorite_color: str\n",
    "\n",
    "# Create a tool that uses ToolRuntime to access user preferences\n",
    "@tool\n",
    "def get_recommendation(runtime: ToolRuntime[UserContext]) -> str:\n",
    "    \"\"\"Get a personalized recommendation based on user preferences.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    favorite_color = runtime.context.favorite_color\n",
    "    return f\"User {user_id} might like items in {favorite_color} color!\"\n",
    "\n",
    "# Create an agent with this tool\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "recommendation_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_recommendation],\n",
    "    system_prompt=\"You are a helpful recommendation assistant\",\n",
    "    context_schema=UserContext\n",
    ")\n",
    "\n",
    "# Invoke with context\n",
    "example_response = recommendation_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What would you recommend for me?\"}]},\n",
    "    context=UserContext(user_id=\"123\", favorite_color=\"blue\")\n",
    ")\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Context-Aware Tools\n",
    "\n",
    "Now create your own personalized greeting tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Alice! How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Create a personalized greeting tool using runtime context\n",
    "# 1. Define a Context dataclass with a user_name field\n",
    "# Hint: Use @dataclass from dataclasses, create a class Context with user_name: str field\n",
    "# Context = None  # TODO: Fill this in (define the dataclass)\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_name: str\n",
    "\n",
    "# 2. Create a tool that uses ToolRuntime to access context\n",
    "# Hint: Use @tool from langchain.tools, function parameter should be runtime: ToolRuntime[Context], access user_name via runtime.context.user_name\n",
    "# get_personalized_greeting = None  # TODO: Fill this in (define the function with @tool decorator)\n",
    "@tool\n",
    "def get_personalized_greeting(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Get a personalized greeting based on user name.\"\"\"\n",
    "    user_name = runtime.context.user_name\n",
    "    return f\"User is {user_name}.\"\n",
    "\n",
    "# 3. Create an agent with this tool\n",
    "# Hint: Use create_agent() with model, tools=[get_personalized_greeting], system_prompt, and context_schema=Context\n",
    "personalized_agent = create_agent(\n",
    "    model = model,\n",
    "    tools = [get_personalized_greeting],\n",
    "    system_prompt = \"You are a helpful assistant\",\n",
    "    context_schema = Context\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 4. Invoke the agent with context\n",
    "# Hint: Use personalized_agent.invoke() with messages and context=Context(user_name=\"Alice\")\n",
    "response = personalized_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"How are you doing?\"}]},\n",
    "    context = Context(user_name = \"Alice\")\n",
    ")  # TODO: Fill this in\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Memory to Agents\n",
    "\n",
    "So far, our agents don't remember previous conversations. Let's add **memory** so agents can maintain context across multiple interactions. A **checkpointer** stores conversation state, which you can think of as the **state of the agent's chain across turns**. LangChain provides:\n",
    "- `InMemorySaver`: For development/testing (lost when program ends)\n",
    "- Database checkpointers: For production (persistent storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: `ToolRuntime` and `InMemorySaver` both relate to “runtime context,” but they operate at different layers.\n",
    ">- `ToolRuntime` is per-tool-call and injected into tool functions; it provides a view of context/memory/config at that moment.\n",
    ">- `InMemorySaver` is storage, passed to the agent as a checkpointer to persist conversation state between invocations (keyed by `thread_id`). It does not get injected into tools.\n",
    ">- `ToolRuntime` doesn’t store anything by itself; `InMemorySaver` doesn’t provide arbitrary runtime context/config—only persistence for state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to add memory to an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: Here's an inspiring quote about success: \"Success is not final, failure is not fatal: it is the courage to continue that counts.\"\n",
      "Second response: I shared the quote: \"Success is not final, failure is not fatal: it is the courage to continue that counts.\"\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Create a checkpointer\n",
    "example_checkpointer = InMemorySaver()\n",
    "\n",
    "# Create a quote tool (different from the fact tool in the exercise)\n",
    "@tool\n",
    "def get_quote(category: str) -> str:\n",
    "    \"\"\"Get an inspirational quote by category.\"\"\"\n",
    "    quotes = {\n",
    "        \"success\": \"Success is not final, failure is not fatal: it is the courage to continue that counts.\",\n",
    "        \"wisdom\": \"The only true wisdom is in knowing you know nothing.\",\n",
    "        \"motivation\": \"The way to get started is to quit talking and begin doing.\"\n",
    "    }\n",
    "    return quotes.get(category.lower(), \"Here's a quote: Keep moving forward!\")\n",
    "\n",
    "# Create an agent with memory\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "quote_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_quote],\n",
    "    system_prompt=\"You are a helpful assistant that shares inspirational quotes\",\n",
    "    checkpointer=example_checkpointer\n",
    ")\n",
    "\n",
    "# Create a config with thread_id\n",
    "example_config = {\"configurable\": {\"thread_id\": \"quote-session-1\"}}\n",
    "\n",
    "# First message\n",
    "example_response1 = quote_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Give me a quote about success\"}]},\n",
    "    config=example_config\n",
    ")\n",
    "\n",
    "# Second message - agent remembers!\n",
    "example_response2 = quote_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What quote did you just share?\"}]},\n",
    "    config=example_config\n",
    ")\n",
    "print(\"First response:\", example_response1['messages'][-1].content)\n",
    "print(\"Second response:\", example_response2['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: The `thread_id` in the config is crucial! It tells the checkpointer which conversation to load. Different `thread_id` values mean different conversations. This allows you to manage multiple concurrent conversations with the same agent.\n",
    "\n",
    "### Exercise 4: Conversational Memory\n",
    "\n",
    "Now create your own agent with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: Here's an interesting fact about Python: it was named after Monty Python's Flying Circus, not the snake!\n",
      "Second response: I shared that Python was named after Monty Python's Flying Circus, not the snake.\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Create an agent with conversational memory\n",
    "# 1. Create an InMemorySaver checkpointer\n",
    "# Hint: Import InMemorySaver from langgraph.checkpoint.memory and create an instance\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "checkpointer = InMemorySaver()  # TODO: Fill this in\n",
    "\n",
    "# 2. Create a simple tool that returns a fact\n",
    "@tool\n",
    "def get_fact(topic: str) -> str:\n",
    "    \"\"\"Get an interesting fact about a topic.\"\"\"\n",
    "    facts = {\n",
    "        \"python\": \"Python was named after Monty Python's Flying Circus\",\n",
    "        \"ai\": \"The term 'artificial intelligence' was coined in 1956\",\n",
    "        \"space\": \"A day on Venus is longer than its year\"\n",
    "    }\n",
    "    return facts.get(topic.lower(), f\"I don't know much about {topic}\")\n",
    "\n",
    "# 3. Create an agent with the checkpointer\n",
    "# Hint: Use create_agent() with model, tools=[get_fact], system_prompt, and checkpointer=checkpointer\n",
    "memory_agent = create_agent(\n",
    "    model = model,\n",
    "    tools = [get_fact],\n",
    "    system_prompt = \"You are a helpful assistant that shares interesting fact\",\n",
    "    checkpointer = checkpointer\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 4. Create a config with a thread_id (this identifies the conversation)\n",
    "# Hint: Create a dictionary with {\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation-1\"}}  # TODO: Fill this in\n",
    "\n",
    "# 5. Ask the agent: \"Tell me a fact about Python\"\n",
    "# Hint: Use memory_agent.invoke() with messages and config\n",
    "response1 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Give me a fact about python\"}]},\n",
    "    config = config\n",
    ")  # TODO: Fill this in\n",
    "print(\"First response:\", response1['messages'][-1].content)\n",
    "\n",
    "# 6. In a follow-up message, ask: \"What was the fact you just told me?\"\n",
    "# Hint: Use memory_agent.invoke() again with the same config - the agent should remember!\n",
    "response2 = response1 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What fact did you just share?\"}]},\n",
    "    config = config\n",
    ")   # TODO: Fill this in\n",
    "print(\"Second response:\", response2['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structured Output\n",
    "\n",
    "Sometimes you want the agent's response in a specific format. LangChain supports **structured output** using dataclasses or `Pydantic` models. Its functional object for this is `ToolStrategy`, which tells the agent to use tools AND return structured output. The agent will still use tools, but format its final response according to your schema. This gives you the best of both worlds - tool usage with predictable output formats.\n",
    "\n",
    "Here's how to create an agent with structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Name: Laptop\n",
      "Price: 999.0\n",
      "Rating: 4.5\n",
      "Features: ['High performance', 'Lightweight design', 'Long battery life']\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define a different response format\n",
    "@dataclass\n",
    "class ProductRecommendation:\n",
    "    \"\"\"Response schema for product recommendations.\"\"\"\n",
    "    product_name: str\n",
    "    price: float\n",
    "    rating: float = 0.0\n",
    "    features: list[str] = field(default_factory=list)\n",
    "\n",
    "# Create a product search tool\n",
    "@tool\n",
    "def search_products(category: str) -> str:\n",
    "    \"\"\"Search for products in a category.\"\"\"\n",
    "    return f\"Found products in {category}: Laptop ($999, 4.5 stars), Tablet ($499, 4.2 stars)\"\n",
    "\n",
    "# Create an agent with structured output\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "product_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[search_products],\n",
    "    system_prompt=\"You are a helpful product recommendation assistant\",\n",
    "    response_format=ToolStrategy(ProductRecommendation)\n",
    ")\n",
    "\n",
    "# Ask a question and get a structured response\n",
    "example_response = product_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Recommend a laptop for me\"}]}\n",
    ")\n",
    "\n",
    "# Access the structured response\n",
    "structured = example_response['structured_response']\n",
    "print(\"Product Name:\", structured.product_name)\n",
    "print(\"Price:\", structured.price)\n",
    "print(\"Rating:\", structured.rating)\n",
    "print(\"Features:\", structured.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Structured Responses\n",
    "\n",
    "Now create your own agent with structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Apple Inc. (AAPL) is currently trading at $180.20 with a P/E ratio of 28.5.\n",
      "Confidence: 1.0\n",
      "Sources: ['internal knowledge base']\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Create an agent with structured output\n",
    "# 1. Define a ResponseFormat dataclass\n",
    "# Hint: Use @dataclass from dataclasses, include answer: str, confidence: float = 0.0, and sources: list[str] = field(default_factory=list)\n",
    "from dataclasses import dataclass, field\n",
    "# ResponseFormat = None  # TODO: Fill this in (define the dataclass)\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for structured output.\"\"\"\n",
    "    answer: str\n",
    "    confidence: float = 0.0\n",
    "    sources: list[str] = field(default_factory = list)\n",
    "\n",
    "# 2. Create a simple tool\n",
    "# Hint: Use @tool from langchain.tools, function should take (query: str) and return a string\n",
    "# search_knowledge_base = None  # TODO: Fill this in (define the function with @tool decorator)\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Searches the internal knowledge base for financial data.\"\"\"\n",
    "    if \"AAPL\" in query.upper():\n",
    "        return \"Apple Inc. (AAPL) is trading at $180.20 with a P/E ratio of 28.5.\"\n",
    "    return \"No specific data found for this ticker.\"\n",
    "\n",
    "# 3. Create an agent with structured output\n",
    "# Hint: Use create_agent() with model, tools, system_prompt, and response_format=ToolStrategy(ResponseFormat) from langchain.agents.structured_output\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "structured_agent = create_agent(\n",
    "    model = model,\n",
    "    tools = [search_knowledge_base],\n",
    "    system_prompt = (\n",
    "        \"You are a financial research assistant. \"\n",
    "        \"When you use a tool, list it in the 'sources' field. \"\n",
    "        \"Set 'confidence' to 1.0 if you found an exact match in the tool, \"\n",
    "        \"and 0.5 if the data is partial or uncertain.\"\n",
    "    ),\n",
    "    response_format = ToolStrategy(ResponseFormat)\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 4. Ask a question and get a structured response\n",
    "# Hint: Use structured_agent.invoke() with messages\n",
    "response = structured_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the current status of AAPL?\"}]}\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# Access the structured response\n",
    "structured = response['structured_response']\n",
    "print(f\"Answer: {structured.answer}\")\n",
    "print(f\"Confidence: {structured.confidence}\")\n",
    "print(f\"Sources: {structured.sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guided Exercise: Daily S&P 500 Decision with Twitter Sentiment Analysis\n",
    "\n",
    "In this exercise, you will build an agent that decides whether to **BUY** or **NOT BUY** units of the S&P 500 (e.g., SPY), computing **local sentiment** for tweets **before the decision day**. We use the following Hugging Face dataset: https://huggingface.co/datasets/StephanAkkerman/stock-market-tweets-data\n",
    "\n",
    "> **Note**: This is a simplified educational example; don’t take it as financial advice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Build a local pandas dataframe (sampled)\n",
    "ds = load_dataset(\"StephanAkkerman/stock-market-tweets-data\", split=\"train\")\n",
    "df = ds.select(range(20000)).to_pandas()\n",
    "\n",
    "# Normalize and parse dates\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\", utc=True)\n",
    "df = df.dropna(subset=[\"created_at\", \"text\"])\n",
    "df[\"created_at_date\"] = df[\"created_at\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentiment score is 0.12075, which is greater than 0.1. Based on this, I recommend a BUY for SPY. The sentiment summary indicates positive sentiment with a positive score of 0.12075.\n"
     ]
    }
   ],
   "source": [
    "# Local sentiment scoring\n",
    "positive_words = {\"gain\", \"gains\", \"bull\", \"bullish\", \"up\", \"upgrade\", \"beat\", \"strong\", \"rally\", \"surge\", \"record\"}\n",
    "negative_words = {\"loss\", \"losses\", \"bear\", \"bearish\", \"down\", \"downgrade\", \"miss\", \"weak\", \"selloff\", \"drop\", \"plunge\"}\n",
    "\n",
    "def sentiment_score(text: str) -> int:\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", text.lower())\n",
    "    score = sum(1 for t in tokens if t in positive_words) - sum(1 for t in tokens if t in negative_words)\n",
    "    return score\n",
    "\n",
    "df[\"sentiment_score\"] = df[\"text\"].fillna(\"\").apply(sentiment_score)\n",
    "\n",
    "# Define the sentiment summary tool\n",
    "# Hint: Use @tool from langchain.tools. Compute statistics from df[\"sentiment_score\"] (e.g. average).\n",
    "@tool\n",
    "def get_sentiment_summary() -> str:\n",
    "    \"\"\"Summarize local tweet sentiment across the full dataset.\"\"\"\n",
    "    return df[\"sentiment_score\"].mean()  # TODO: Fill this in\n",
    "\n",
    "# Write the SYSTEM_PROMPT\n",
    "# Hint: Include goals + rules; Summary must include average sentiment score and key themes.\n",
    "SYSTEM_PROMPT = \"\"\"You are a Financial Analyst Agent. \n",
    "    Your goal is to provide a trading recommendation (BUY or NOT BUY) based on sentiment data.\n",
    "    Rules:\n",
    "    1. Always call the 'get_sentiment_summary' tool first.\n",
    "    2. If the Average Sentiment Score is greater than 0.1, recommend 'BUY'.\n",
    "    3. Otherwise, recommend 'NOT BUY'.\n",
    "    4. Include the average score and the counts of positive/negative sentiment in your final answer.\"\"\" # TODO: Fill this in\n",
    "\n",
    "# Initialize the model\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Create the agent and run it\n",
    "# Hint: create_agent(model=example_model, tools=[get_sentiment_summary], system_prompt=SYSTEM_PROMPT)\n",
    "decision_agent = create_agent(\n",
    "    model = example_model,\n",
    "    tools = [get_sentiment_summary],\n",
    "    system_prompt = SYSTEM_PROMPT,\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "response = decision_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Use the overall tweet sentiment to decide BUY or NOT BUY SPY.\"}]})\n",
    "print(response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congratulations \n",
    "You've completed the LangChain tutorial! We covered\n",
    "\n",
    "- How to create agents with LangChain  \n",
    "- How to build custom tools  \n",
    "- How to add memory to agents  \n",
    "- How to use structured output  \n",
    "- How to build a daily decision agent  \n",
    "\n",
    "### Possible next steps to explore\n",
    "   - **LangGraph**: For more complex agent workflows (see the LangGraph notebook!)\n",
    "   - **Retrieval**: Connect agents to vector databases for RAG\n",
    "   - **Multi-agent systems**: Agents that collaborate\n",
    "   - **LangSmith**: Observability and debugging tools\n",
    "\n",
    "### Additional resources\n",
    "   - [LangChain Docs](https://docs.langchain.com)\n",
    "   - [LangChain Quickstart](https://docs.langchain.com/oss/python/langchain/quickstart)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Happy learning!\n",
    "\n",
    "Pedro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-chain)",
   "language": "python",
   "name": "lang-chain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
